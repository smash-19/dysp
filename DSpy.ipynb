{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM392ed4SmhQ"
      },
      "source": [
        "***Python*** ***Assignment***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NqdU2UodIaQ",
        "outputId": "8678be37-6e20-4570-8a42-6e169f2d6569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.2 requires grpcio>=1.71.2, but you have grpcio 1.67.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ… Environment Ready.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q dspy-ai>=2.5.0 anthropic pydantic beautifulsoup4 requests pandas\n",
        "\n",
        "import dspy\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "print(\"âœ… Environment Ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_A2q5RedkKg",
        "outputId": "eef83182-81be-4a3c-ff74-1d2db011acc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Connected to: anthropic/claude-sonnet-4-20250514\n"
          ]
        }
      ],
      "source": [
        "# @title 2. Secure API Configuration\n",
        "CONFIRMED_MODEL = \"anthropic/claude-sonnet-4-20250514\"\n",
        "\n",
        "try:\n",
        "    api_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "    # api_base = userdata.get('LONGCAT_BASE_URL') # Uncomment if using proxy\n",
        "    api_base = None\n",
        "\n",
        "    lm = dspy.LM(\n",
        "        model=CONFIRMED_MODEL,\n",
        "        api_key=api_key,\n",
        "        api_base=api_base,\n",
        "        temperature=0.0\n",
        "    )\n",
        "    dspy.configure(lm=lm)\n",
        "    print(f\"âœ… Connected to: {CONFIRMED_MODEL}\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"âŒ Error: 'ANTHROPIC_API_KEY' is missing from Secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Connection Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56pZuXpAdn3v",
        "outputId": "ea01f36a-fc6b-4fe6-8335-0ba5cc5f5bea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DSPy Modules Initialized.\n"
          ]
        }
      ],
      "source": [
        "# @title 3. Define Pydantic Models & DSPy Signatures\n",
        "\n",
        "# 1. Entity Extraction\n",
        "class EntityItem(BaseModel):\n",
        "    name: str = Field(description=\"Exact entity name (e.g. 'pea-barley intercrop').\")\n",
        "    category: str = Field(description=\"Semantic category (e.g. Crop, Process, Concept).\")\n",
        "\n",
        "class EntityList(BaseModel):\n",
        "    entities: List[EntityItem]\n",
        "\n",
        "class ExtractEntitiesSig(dspy.Signature):\n",
        "    \"\"\"Extract key domain entities from text. Return structured JSON.\"\"\"\n",
        "    text: str = dspy.InputField()\n",
        "    output: EntityList = dspy.OutputField()\n",
        "\n",
        "# 2. Deduplication\n",
        "class DeduplicationOutput(BaseModel):\n",
        "    deduplicated_entities: List[EntityItem]\n",
        "    confidence_score: float = Field(description=\"Confidence score (0.0 - 1.0).\")\n",
        "\n",
        "class DeduplicateSig(dspy.Signature):\n",
        "    \"\"\"Merge synonyms and remove duplicates. Provide confidence score.\"\"\"\n",
        "    raw_entities: List[EntityItem] = dspy.InputField()\n",
        "    output: DeduplicationOutput = dspy.OutputField()\n",
        "\n",
        "# 3. Knowledge Graph\n",
        "class Relationship(BaseModel):\n",
        "    source: str = Field(description=\"Source node (must be from valid_entities).\")\n",
        "    target: str = Field(description=\"Target node (must be from valid_entities).\")\n",
        "    label: str = Field(description=\"Edge label (max 40 chars).\")\n",
        "\n",
        "class GraphOutput(BaseModel):\n",
        "    relationships: List[Relationship]\n",
        "\n",
        "class BuildGraphSig(dspy.Signature):\n",
        "    \"\"\"Identify relationships using ONLY the provided valid entities.\"\"\"\n",
        "    text: str = dspy.InputField()\n",
        "    valid_entities: List[str] = dspy.InputField()\n",
        "    output: GraphOutput = dspy.OutputField()\n",
        "\n",
        "# Initialize Predictors\n",
        "extractor = dspy.Predict(ExtractEntitiesSig)\n",
        "deduplicator = dspy.Predict(DeduplicateSig)\n",
        "graph_builder = dspy.Predict(BuildGraphSig)\n",
        "\n",
        "print(\"âœ… DSPy Modules Initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYatUETqdqZG",
        "outputId": "f61467af-e50e-4a5e-f7d6-aa61e1a45400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Utilities Ready.\n"
          ]
        }
      ],
      "source": [
        "# @title 4. Define Utilities\n",
        "\n",
        "def scrape_and_clean(url: str) -> str:\n",
        "    \"\"\"Robust scraper with noise removal.\"\"\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Kill noise\n",
        "        for el in soup(['script', 'style', 'nav', 'footer', 'header', 'form', 'iframe', 'aside']):\n",
        "            el.decompose()\n",
        "\n",
        "        text = re.sub(r'\\s+', ' ', soup.get_text(separator=' ')).strip()\n",
        "\n",
        "        return text[:20000]\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Scrape Error for {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def generate_mermaid(rels) -> str:\n",
        "    \"\"\"Convert relationships to Mermaid Syntax.\"\"\"\n",
        "    lines = [\"graph TD\"]\n",
        "    for r in rels:\n",
        "        s = r.source.replace('\"', '').replace('(', '').replace(')', '')\n",
        "        t = r.target.replace('\"', '').replace('(', '').replace(')', '')\n",
        "        l = r.label.replace('\"', '')[:40]\n",
        "        lines.append(f'    \"{s}\" -- \"{l}\" --> \"{t}\"')\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def robust_deduplicate_logic(entities):\n",
        "    \"\"\"Wrapper for deduplication predictor.\"\"\"\n",
        "    if not entities: return []\n",
        "    try:\n",
        "        res = deduplicator(raw_entities=entities).output\n",
        "        print(f\"      > Dedup Confidence: {res.confidence_score}\")\n",
        "        return res.deduplicated_entities\n",
        "    except:\n",
        "        return entities\n",
        "\n",
        "print(\"âœ… Utilities Ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xJgP6t5ds5k",
        "outputId": "e762787d-c14b-4980-acab-5c30e26ab44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Starting Batch Processing...\n",
            "============================================================\n",
            "\n",
            "ðŸ”— Processing 1/10: https://en.wikipedia.org/wiki/Sustainable_agriculture\n",
            "      > Extracted 51 raw entities\n",
            "      > Dedup Confidence: 0.95\n",
            "      > Cleaned to 49 unique entities\n",
            "      > Saved mermaid_1.md\n",
            "\n",
            "ðŸ”— Processing 2/10: https://www.nature.com/articles/d41586-025-03353-5\n",
            "      > Extracted 15 raw entities\n",
            "      > Dedup Confidence: 0.95\n",
            "      > Cleaned to 15 unique entities\n",
            "      > Saved mermaid_2.md\n",
            "\n",
            "ðŸ”— Processing 3/10: https://www.sciencedirect.com/science/article/pii/S1043661820315152\n",
            "âš ï¸ Scrape Error for https://www.sciencedirect.com/science/article/pii/S1043661820315152: 403 Client Error: Forbidden for url: https://www.sciencedirect.com/science/article/pii/S1043661820315152\n",
            "      > âš ï¸ Skipping (Scrape failed)\n",
            "\n",
            "ðŸ”— Processing 4/10: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\n",
            "âš ï¸ Scrape Error for https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/: 403 Client Error: Forbidden for url: https://pmc.ncbi.nlm.nih.gov/articles/PMC10457221/\n",
            "      > âš ï¸ Skipping (Scrape failed)\n",
            "\n",
            "ðŸ”— Processing 5/10: https://www.fao.org/3/y4671e/y4671e06.htm\n",
            "      > Extracted 55 raw entities\n",
            "      > Dedup Confidence: 0.95\n",
            "      > Cleaned to 54 unique entities\n",
            "      > Saved mermaid_5.md\n",
            "\n",
            "ðŸ”— Processing 6/10: https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\n",
            "âš ï¸ Scrape Error for https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria: 403 Client Error: Forbidden for url: https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\n",
            "      > âš ï¸ Skipping (Scrape failed)\n",
            "\n",
            "ðŸ”— Processing 7/10: https://www.sciencedirect.com/science/article/pii/S0378378220307088\n",
            "âš ï¸ Scrape Error for https://www.sciencedirect.com/science/article/pii/S0378378220307088: 403 Client Error: Forbidden for url: https://www.sciencedirect.com/science/article/pii/S0378378220307088\n",
            "      > âš ï¸ Skipping (Scrape failed)\n",
            "\n",
            "ðŸ”— Processing 8/10: https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\n",
            "      > Extracted 22 raw entities\n",
            "      > Dedup Confidence: 0.95\n",
            "      > Cleaned to 20 unique entities\n",
            "      > Saved mermaid_8.md\n",
            "\n",
            "ðŸ”— Processing 9/10: https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\n",
            "âš ï¸ Scrape Error for https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7: 403 Client Error: Forbidden for url: https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\n",
            "      > âš ï¸ Skipping (Scrape failed)\n",
            "\n",
            "ðŸ”— Processing 10/10: https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\n",
            "      > Extracted 28 raw entities\n",
            "      > Dedup Confidence: 0.95\n",
            "      > Cleaned to 27 unique entities\n",
            "      > Saved mermaid_10.md\n",
            "============================================================\n",
            "ðŸŽ‰ SUCCESS! Generated 'tags.csv' (165 rows) and 10 Mermaid files.\n"
          ]
        }
      ],
      "source": [
        "# @title 5. Execute Full Pipeline\n",
        "urls = [\n",
        "    \"https://en.wikipedia.org/wiki/Sustainable_agriculture\",\n",
        "    \"https://www.nature.com/articles/d41586-025-03353-5\",\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S1043661820315152\",\n",
        "    \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\",\n",
        "    \"https://www.fao.org/3/y4671e/y4671e06.htm\",\n",
        "    \"https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\",\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S0378378220307088\",\n",
        "    \"https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\",\n",
        "    \"https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\",\n",
        "    \"https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "print(\"ðŸš€ Starting Batch Processing...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, url in enumerate(urls):\n",
        "    print(f\"\\nðŸ”— Processing {i+1}/10: {url}\")\n",
        "\n",
        "    # 1. Scrape\n",
        "    text = scrape_and_clean(url)\n",
        "    if not text:\n",
        "        print(\"      > âš ï¸ Skipping (Scrape failed)\")\n",
        "        continue\n",
        "\n",
        "    # 2. Extract\n",
        "    try:\n",
        "        raw_ents = extractor(text=text).output.entities\n",
        "        print(f\"      > Extracted {len(raw_ents)} raw entities\")\n",
        "    except Exception as e:\n",
        "        print(f\"      > Extraction Failed: {e}\")\n",
        "        continue\n",
        "\n",
        "    # 3. Deduplicate\n",
        "    clean_ents = robust_deduplicate_logic(raw_ents)\n",
        "    print(f\"      > Cleaned to {len(clean_ents)} unique entities\")\n",
        "\n",
        "    # 4. Graph Generation\n",
        "    try:\n",
        "        valid_names = [e.name for e in clean_ents]\n",
        "        rels = graph_builder(text=text[:15000], valid_entities=valid_names).output.relationships\n",
        "        mermaid_code = generate_mermaid(rels)\n",
        "\n",
        "        # Save Mermaid File\n",
        "        fname = f\"mermaid_{i+1}.md\"\n",
        "        with open(fname, \"w\") as f:\n",
        "            f.write(mermaid_code)\n",
        "        print(f\"      > Saved {fname}\")\n",
        "    except Exception as e:\n",
        "        print(f\"      > Graph Generation Failed: {e}\")\n",
        "\n",
        "    # 5. Collect CSV Data\n",
        "    for e in clean_ents:\n",
        "        results.append({\"link\": url, \"tag\": e.name, \"tag_type\": e.category})\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "# 6. Export CSV\n",
        "print(\"=\"*60)\n",
        "if results:\n",
        "    df = pd.DataFrame(results)\n",
        "    df.drop_duplicates(subset=['link', 'tag'], inplace=True)\n",
        "    df.to_csv(\"tags.csv\", index=False)\n",
        "    print(f\"ðŸŽ‰ SUCCESS! Generated 'tags.csv' ({len(df)} rows) and 10 Mermaid files.\")\n",
        "else:\n",
        "    print(\"âš ï¸ No data collected.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK61du5IdupD",
        "outputId": "1e4a476b-a914-4f05-b32e-1d39b3305a79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### File Check ###\n",
            "Mermaid Files Found: 5\n",
            "CSV Found: True\n",
            "\n",
            "### CSV Preview ###\n",
            "                                                link                      tag  \\\n",
            "0  https://en.wikipedia.org/wiki/Sustainable_agri...  sustainable agriculture   \n",
            "1  https://en.wikipedia.org/wiki/Sustainable_agri...       shade-grown coffee   \n",
            "2  https://en.wikipedia.org/wiki/Sustainable_agri...              polyculture   \n",
            "3  https://en.wikipedia.org/wiki/Sustainable_agri...       ecosystem services   \n",
            "4  https://en.wikipedia.org/wiki/Sustainable_agri...           climate change   \n",
            "\n",
            "               tag_type  \n",
            "0               Concept  \n",
            "1                  Crop  \n",
            "2               Process  \n",
            "3               Concept  \n",
            "4  Environmental Factor  \n",
            "\n",
            "### mermaid_8.md Preview ###\n",
            "graph TD\n",
            "    \"rectangular telescope\" -- \"could help discover\" --> \"Earth 2.0\"\n",
            "    \"rectangular telescope\" -- \"modeled after\" --> \"Diffractive Interfero Coronagraph Exoplanet Resolver\"\n",
            "    \"rectangular telescope\" -- \"similar size to\" --> \"James Webb Space Telescope\"\n",
            "    \"rectangular telescope\" -- \"can detect\" --> \"exoplanet\"\n",
            "    \"rectangular telescope\" -- \"operates at\" --> \"infrared wavelength\"\n",
            "    \"rectangular telescope\" -- \"uses\" --> \"10 micron wavelength\"\n",
            "    \"habitable worlds\" -- \"require\" --> \"liquid water\"\n",
            "    \"liquid water\" -- \"enables formation of\" --> \"multicellular life\"\n",
            "    \"Earth-like exoplanet\" -- \"orbits\" --> \"sun-like stars\"\n",
            "    \"Earth-like exoplanet\" -- \"may contain\" --> \"liquid water\"\n",
            "    \"starshade\" -- \"helps detect\" --> \"exoplanet\"\n",
            "    \"telescope resolution\" -- \"required to separate\" --> \"exoplanet\"\n",
            "    \"oxygen\" -- \"formed through\" --> \"photosynthesis\"\n",
            "    \"oxygen\" -- \"indicates presence of\" --> \"extraterrestrial life\"\n",
            "    \"space probe\" -- \"could be sent to\" --> \"Earth 2.0\"\n",
            "    \"Rensselaer Polytechnic Institute\" -- \"published research in\" --> \"Frontiers in Astronomy and Space Sciences\"\n"
          ]
        }
      ],
      "source": [
        "# @title 6. Preview Output\n",
        "import os\n",
        "\n",
        "print(\"### File Check ###\")\n",
        "files = os.listdir('.')\n",
        "md_files = [f for f in files if f.startswith('mermaid_') and f.endswith('.md')]\n",
        "print(f\"Mermaid Files Found: {len(md_files)}\")\n",
        "print(f\"CSV Found: {'tags.csv' in files}\")\n",
        "\n",
        "if 'tags.csv' in files:\n",
        "    print(\"\\n### CSV Preview ###\")\n",
        "    print(pd.read_csv(\"tags.csv\").head())\n",
        "\n",
        "if md_files:\n",
        "    print(f\"\\n### {md_files[0]} Preview ###\")\n",
        "    with open(md_files[0], 'r') as f:\n",
        "        print(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pAQWI5wdetoW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}